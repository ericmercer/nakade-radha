\section{Related Work}

Different parallel programming models have been created with different properties such as task interactions, task granularity,  etc. Some examples of these models are message passing, data parallelism, task parallelism. Message passing \cite{Forum:1994:MMI:898758} programs create multiple tasks with each encapsulating some local data. Data is shared between the tasks by sending messages from one task to another. Data parallelism refers to application of same instruction to multiple elements of data. Task parallelism is achieved by executing different instructions concurrently on multiple sets of data.

In this research we are mainly going to focus on task parallel programs. Various task parallel languages have been developed such as Habanero Java, Cilk, X10, Chapel, OpenMP 3.0 etc. Habanero Java \cite{cave2011habanero} is a task parallel programming language developed at the Rice University. It was developed as an extension to X10 with particular emphasis on safety properties of parallel constructs. The HJ compiler generates standard Java class files that can run on any JVM. The HJ runtime is responsible for orchestrating the creation, execution and termination of HJ tasks. A Java 8 library implementation for this language, known as HJLib \cite{imam2014habanero} has also been created. This library makes extensive use of lambda expressions and can run on any Java 8 JVM. HJ programs provide various safety guarantees if the parallel programming constructs are used correctly. Verifying HJ programs using tools such as Java Path Finder (JPF) can be time and memory consuming because of the numerous JPF state expansions. Hence, an HJ verification runtime (VR) \cite{anderson2014jpf} was developed by Anderson et al. to use JPF for verifying HJ programs. This runtime provides a lightweight alternative to verifying HJ programs using JPF. JPF \cite{JPF} is a highly customizable execution environment for verification of Java bytecode programs. It is an explicit-state model checker for Java programs. It collects deep runtime information like coverage metrics. It can be used to detect concurrency errors such as deadlocks, data-races etc. Although, as the program size increases, there is an exponential growth in the size of the state space that needs to be explored by JPF. Hence, JPF is not suitable to verify very-large concurrent systems.

X10 \cite{charles2005x10} was developed at IBM as a part of the IBM PERCS project (Productive Easy-to-use Reliable Computer Systems). X10 is a type-safe, parallel, distributed object oriented language with support for high performance computation over distributed multi-dimensional arrays. Gligoric et al. extended the model checker JPF in \cite{gligoric2012x10x} to verify X10 programs and detect concurrency related bugs. Chapel programming language \cite{chamberlain2007parallel} was developed as a part of DARPA's High Productivity Computing Systems (HPCS) program. The Chapel provides higher-level abstractions for parallelism using anonymous threads that are implemented by the compiler and runtime system. Chapel programs are subject to concurrency problems such as deadlocks, race conditions etc. To verify the correctness of chapel programs, Zirkel et al. developed a tool for model checking and symbolic execution of chapel programs \cite{zirkel2013automated}. OpenMP \cite{dagum1998openmp} is a set of compiler directives and callable runtime library routines for Fortran and C to express shared-memory parallelism. Any sequential program written in C or Fortran can be easily parallelized using OpenMP. Another C-based runtime system for multi-threaded parallel programming is Cilk \cite{blumofe1996cilk} . A Cilk program consists of procedures that can be broken down into a sequence of threads. The performance of Cilk programs can be modeled accurately. This provides the developers a way to improve performance of their programs.

Model checking suffers from an inherent shortcoming. The exponential growth in the state space of the program being verified makes model checking unsuitable for large programs. A lot of methods are being developed such as partial order reductions that help to reduce the state space that needs to be explored for finding bugs such as data races that can be detected only under certain thread interleavings. Also, some errors are dependent on the control flow structure of the program. These errors are detected only if a given input takes that particular branch of the program on which the error exists. To detect such errors we have to test all the branches of the program. Concolic execution provides a way of detecting errors on all possible branches of the program. Concolic execution automates test input generation by combining the concrete and symbolic execution of the code under test. Concolic execution couples both concrete and symbolic execution by running both of them simultaneously such that each gets feedback from the other.

Task parallel languages achieve parallelism by distributing execution processes across different parallel computing nodes. A formal model is helpful to describe the properties of task parallel programming languages. 
\\
\\
\textbf{Semantics of parallel programming languages:} 

Creating semantic models of programming languages helps to reason about the properties and performance of the various constructs of programming languages. This is an important step in the verification process of programming languages.

Emmi and Boujjani introduced an interleaving free model of isolated hierarchical parallel computations for expressing general parallel programming languages \cite{bouajjani2012analysis}. They formalized a system for measuring the complexity of deciding state reachability for finite-data recursive programs. Another way of creating formal models of programming languages is through Redex \cite{klein2012run}. Redex is an executable domain-specific language for mechanizing semantic models developed by PLT. These models can be used to state theorems about the models and prove them. Redex is used by semantics engineers to formulate the syntax and semantics of the model, create test suites, run randomized testing and use graphical tools for visualizing examples etc.
\\
\\
\textbf{Formalism of properties of Parallel Programming languages:}

Scott and Lu have proposed various history-based definitions of determinism in \cite{lu2011toward}. They have discussed the comparative advantages of these properties. They have also discussed the containment relationships for these properties. Dennis, Gao and Sarkar presented precise definitions of the two related properties of program schemata - ‘determinacy’ and ‘repeatability’ in \cite{dennis2012determinacy}. A key advantage of providing definitions for schemata rather than concrete programs is that it simplifies the task for programmers and tools to check these properties. The definitions of these properties are provided for schemata arising from data flow programs and task-parallel programs, thereby also establishing new relationships between the two models.

Race conditions occur in shared-memory parallel programs when accesses to shared-memory are not synchronized. Netzer and Miller formalized the definitions of  race conditions occurring in shared-memory parallel programs \cite{netzer1992race}. The race conditions are divided broadly into two categories - general races that cause deterministic programs to fail in execution and data races that appear in non-deterministic programs. Banerjee et al. developed a rigorous mathematical framework that can be used to study the trade-off between the amount of access history kept and the kinds of data races that can be detected \cite{banerjee2006theory}. Using this framework, they developed some algorithms for data race detection under different conditions.

Bocchino et al. developed a region-based type and effect system for expressing important patterns of deterministic parallelism in imperative, object-oriented programs \cite{bocchino2009type}. This system simplifies parallel programming by guaranteeing deterministic semantics with modular, compile time type checking. Kahlon and Wang proposed a concept of Universal Causality Graphs (UCG) in \cite{kahlon2010universal}. UCGs encode the set of all feasible interleavings that a given correctness property may violate. UCGs provide a unified happens-before model by capturing causality constraints imposed by the property at hand as well as scheduling constraints imposed by synchronization primitives as causality constraints.

Using these formal definitions of various properties, various tools were developed for data-race detection, deadlock detection, checking determinism etc.\\
\\
\textbf{Checking Determinism: }

In a parallel program, the threads of the parallel program can be interleaved non-deterministically during execution. Different thread interleavings result in different outputs for the same program input. Some of the results produced by such interleavings can be correct while others are wrong. Parallel programs should always produce the correct result irrespective of the thread interleavings that occur during program execution.

Miller and Choi implemented an integrated debugging system for parallel programs in \cite{miller1988mechanism}. They created dynamic program dependence graphs that show the causal relations between program events. The dynamic program dependence graph is created by observing a trace of the parallel program execution. They also showed how the dynamic program dependence graph can be used to detect data-races in parallel programs. Burnim and Sen created an assertion framework that can be used to specify pairs of program state that can arise due to non-deterministic thread inter-leavings\cite{burnim2009asserting}. Such pairs of program state result in a deterministic result in spite of the different parallel schedules. They created a Java library that can be used to specify these assertions. They also created an algorithm called Determin \cite{burnim2010determin} that can dynamically infer a likely deterministic specification when provided with a set of inputs and schedules. Insta-check \cite{nistor2010instantcheck} is another technique for checking external determinism during testing of parallel programs. It checks whether different runs of a parallel program with same input produce different outputs. This is done by computing a 64-bit hash of the memory state during program run. If two program runs with same input produce different hashes, then insta-check reports that the program is non-deterministic. Vechev et al. developed a static analysis technique for automatic verification of determinism in parallel programs \cite{vechev2011automatic}. The analysis is done in two phases. The first phase identifies parts of the parallel program that run in parallel. Each part is sequentially analyzed by assuming that all memory locations accessed by the task are independent from locations accessed by other tasks that are running in parallel. In the second phase, the analysis checks whether this independence assumption holds i.e. all memory accesses are independent.

The main cause of non-determinism in parallel programs is data races and deadlocks that arise during different schedules of the program. To prove program correctness of parallel programs, it is important to detect all data races and deadlocks.
\\
\\
\textbf{Data Race and Deadlock Detection: }

Data races occur in parallel programs when two or more threads access a memory location and at least one of the accesses is a ‘write’.  It is very difficult to detect data races in concurrent programs. Deadlocks cause the programs to stall. A number of researchers have worked on data race and deadlock detection.

Savage et al. developed a tool called Eraser \cite{savage1997eraser} to dynamically detect data races in multi-threaded programs. Eraser uses binary rewriting techniques to monitor every shared-memory reference and verify that consistent locking behavior is used. A method to perform static data race detection in concurrent C programs was developed by Kahlon et al. This method \cite{kahlon2009static} involved creating a precise context-sensitive concurrent control flow graph. Using this graph, identify the shared variables and lock pointers, compute on initial database of race warnings and then prune away the spurious messages using may-happen-in-parallel (MHP) analysis. Flanagan and Freund developed a precise data race detection tool called FastTrack \cite{flanagan2009fasttrack}. It uses an adaptive lightweight representation for the happens-before relation that reduces both time and space overheads. Engler and Aashcraft developed a static tool called RacerX for detection of deadlocks and race conditions \cite{engler2003racerx}. The tool is specifically designed for checking large multi-threaded systems. It has been applied to Linux, FreeBSD etc. for detecting concurrency related errors in these complex systems. 