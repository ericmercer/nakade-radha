verify \section{Introduction}

Until recently, the speed of the processors was expected to increase rapidly over time with sustained technology advances, and the motivation for parallel computing was low. But now, clock frequencies for individual processors are no longer increasing. The reason for that being the power consumed by a processor using current device technologies varies as the cube of the frequency. Processor chips in laptop and personal computers are typically limited to consume at most 75 Watts because a larger power consumption would lead to chips overheating and malfunctioning, given the limited air-cooling capacity of these computers. This power limitation has been referred to as the ''Power Wall''. The Power Wall is even more critical in smart-phones and hand held devices because larger power consumption leads to a shorter battery life. Therefore, clock frequency can no longer be increased. When multiple cores are used in parallel, the speed of computation is increased but not the power consumption. This is the main motivation behind parallel programming.

 However, the introduction of concurrency leads to non deterministic behavior of programs. When programs execute different instructions simultaneously, different thread-schedules and memory accesses patterns are observed. If two or more processes executing in parallel access a common memory location and atleast one of them is a write, then a data race will occur. Data-race means that the value of the memory location will be dependent on the order of the instructions that accessed it and it is hard to determine the exact value of that memory location at the end of the execution of that task. Another problem that can arise when two or more tasks execute in parallel is deadlock. When a task is waiting to acquire a resource held by another task and the other task is waiting to acquire a resource held by the first task, then both tasks don't make any progress. This results in the execution of the program getting stalled. Many tools have been developed to detect deadlocks and data races in parallel programs. These tools assist the developers of concurrent programs to detect erroneous behavior of their programs and rectify their implementations. However, in large systems it takes a great deal of effort to locate and rectify such errors. Especially in safety-critical systems, it is very important to ensure that the concurrent programs are free of such errors. The developers are, therefore, trying to develop parallel programming languages that ensure safety against concurrency related errors. 
 
 The research proposed here discusses a new way of verification of task parallel programming languages with the help of computation graphs. A computation graph of a program represents an execution of the program under certain thread-interleavings. A CG is an acyclic directed graph that consists of a set of nodes, where each node represents a step consisting of some sequential execution of the program and a set of edges that represent the ordering of the steps. A CG should store the memory locations accessed and updated by each of the operators. It should also correctly reflect the control flow structure of the program. These properties are necessary for verification algorithms to correctly identify the errors in the programming constructs. Different computation graph structures are created for different inputs of the same program because of the control flow of the program. If all these computation graphs can satisfy the safety properties, then only we can claim that the program is safe to execute with any input. To enumerate all the computation graph structures of the program, we have to know all the possible inputs that can create these different structures. One way to do this is using concolic execution. Concolic execution is a hybrid verification technique that performs symbolic execution (treating program variables as symbols) with concrete execution (testing on particular inputs).